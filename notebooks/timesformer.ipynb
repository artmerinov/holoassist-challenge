{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artemmerinov/PycharmProjects/holoassist/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import TimesformerModel, TimesformerForVideoClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesformer_model = TimesformerForVideoClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"facebook/timesformer-base-finetuned-k400\",\n",
    "    cache_dir=\"/Users/artemmerinov/data/backbones/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[ 2.8264e+00,  9.0088e-01,  9.3344e-01, -2.1993e+00,  1.1449e+00,\n",
       "          1.1901e+00, -3.2675e+00,  2.4619e-01, -1.3535e+00, -2.0944e+00,\n",
       "         -8.3951e-01, -7.9373e-01,  1.0570e-01, -3.8395e-01, -2.5643e+00,\n",
       "         -2.7090e+00,  4.1452e+00, -3.2700e-01,  2.8708e-01, -5.3996e-01,\n",
       "         -5.6691e-01, -1.5525e-02,  9.3082e-01,  3.0823e+00,  4.5135e-01,\n",
       "          5.2683e-01,  2.5227e-01, -4.9261e-02,  2.0034e-01,  8.1244e-01,\n",
       "         -6.8982e-01, -2.7789e+00, -8.1570e-01, -1.3574e+00, -1.6898e+00,\n",
       "          2.6373e+00,  1.1027e+00,  1.1494e+00, -2.0442e+00, -2.1983e+00,\n",
       "          6.2341e+00, -1.7176e+00, -9.7565e-01, -6.3785e-01,  1.9844e+00,\n",
       "         -5.6728e-01,  6.6342e-01, -2.5235e+00,  1.3755e+00,  1.1259e+00,\n",
       "         -9.2174e-01, -4.6059e-01, -1.7260e+00, -2.6628e+00,  2.7322e-01,\n",
       "         -1.7522e+00, -1.6546e+00, -6.4313e-01, -5.8806e-01, -2.4012e+00,\n",
       "          5.8584e-01, -3.5544e-01,  2.1017e-01,  7.3569e-01, -3.2433e-01,\n",
       "         -7.9613e-01, -9.7387e-01,  1.4373e+00,  1.5049e+00,  2.1492e+00,\n",
       "         -1.5821e+00, -1.0280e+00, -7.0918e-01, -9.4553e-01, -9.4349e-01,\n",
       "         -1.8041e+00,  2.6352e+00, -4.9332e-01, -1.1480e+00,  1.6319e+00,\n",
       "         -1.0901e+00,  1.0642e+00, -7.1646e-01,  2.6728e-01,  1.5914e+00,\n",
       "          1.7041e+00, -2.9416e+00, -9.1101e-02, -1.9310e+00, -1.9544e+00,\n",
       "         -1.0957e+00, -1.7752e-01, -6.7134e-01,  1.2844e+00, -4.8367e-01,\n",
       "          7.3312e-01,  1.3042e+00,  1.4190e+00,  3.8661e+00,  5.8214e-01,\n",
       "         -8.8776e-01, -2.9389e-01, -2.0869e+00, -1.6686e+00, -1.0474e+00,\n",
       "         -1.3999e+00,  1.6761e+00,  1.2127e+00, -1.3229e+00, -7.2113e-01,\n",
       "         -9.8079e-01,  2.4621e+00, -1.7843e+00,  1.8962e-01,  5.8845e-01,\n",
       "         -1.5891e+00, -3.3818e-01, -2.3083e-01, -3.4124e+00,  1.3575e+00,\n",
       "          2.2942e+00,  9.4170e-01, -5.7911e-01,  2.2470e+00,  2.5980e-02,\n",
       "         -1.8227e-01,  2.4053e+00,  5.5506e+00,  1.2131e+00, -2.5156e-01,\n",
       "          2.8381e+00, -1.0807e+00, -5.3674e-01,  4.6151e-01,  6.8559e-01,\n",
       "          1.1504e-02, -2.4042e+00, -9.3244e-01,  6.0147e-01, -6.0041e-01,\n",
       "          4.3614e-01, -7.6434e-01, -2.3527e+00,  8.7516e-01, -2.3268e+00,\n",
       "         -6.0778e-01, -1.0944e+00, -8.6847e-01, -2.4398e-01,  1.3270e+00,\n",
       "          1.1516e+00,  5.1001e-01, -9.6276e-01,  7.0939e-01,  2.1848e+00,\n",
       "         -3.8179e-01, -8.9217e-01,  1.9900e+00, -2.9562e+00,  1.1728e+00,\n",
       "         -1.1653e+00, -2.0718e+00,  1.4562e+00, -4.0355e-01,  2.7534e+00,\n",
       "          1.3659e+00, -5.5696e-01,  4.2690e-01,  4.3526e-01,  9.9901e-01,\n",
       "         -1.5044e-01,  8.8129e-02, -2.1689e+00, -2.0541e+00, -1.9896e+00,\n",
       "          4.4720e+00, -2.0750e+00,  1.4974e+00,  1.1079e+00, -7.7835e-01,\n",
       "          1.7564e+00, -2.3235e+00, -2.0234e+00,  5.8527e-02, -2.3355e+00,\n",
       "          5.3851e-01, -2.3325e-01,  2.1728e+00,  7.1984e-01,  5.0834e-01,\n",
       "         -1.4064e+00, -3.2631e-01, -5.7539e-01,  5.6644e-01, -3.5984e-02,\n",
       "          2.5449e+00,  4.6799e-01,  3.3151e-01, -1.6407e+00, -4.5391e-01,\n",
       "         -1.1420e+00, -1.3999e-01, -8.0523e-01,  1.1763e+00, -3.7399e+00,\n",
       "          1.0172e+00,  5.1215e+00, -7.1961e-01, -1.5515e+00, -2.0660e+00,\n",
       "          7.3282e-01, -3.6505e-01, -1.1548e+00,  3.1378e-01,  2.7331e-02,\n",
       "         -2.7737e+00, -2.4683e-01,  6.8228e-02, -2.4678e+00,  2.2625e+00,\n",
       "          1.1476e+00, -2.7689e-02,  3.5653e+00,  7.4782e-01, -1.5896e+00,\n",
       "          1.4995e+00, -1.7646e+00, -1.7818e+00,  1.7535e-01, -4.6035e-01,\n",
       "          4.1455e-01, -1.3853e+00,  1.7015e+00, -5.7034e-02,  6.8063e-01,\n",
       "          4.3848e-01,  1.1042e+00, -1.6221e+00, -1.6226e+00,  1.9889e+00,\n",
       "          5.5095e-01,  4.7694e-01, -1.6184e+00, -2.7465e-01,  1.4995e+00,\n",
       "         -2.3968e-01, -1.7827e+00,  1.7886e+00,  1.3058e+00,  7.0499e-01,\n",
       "          1.2153e+00, -3.3227e+00,  1.6714e+00, -2.2006e+00, -2.5225e+00,\n",
       "         -2.7081e-01,  7.5217e-01, -2.2687e+00,  1.2994e+00, -1.9058e+00,\n",
       "          3.0392e-01,  1.7117e+00,  1.4700e+00,  6.2463e-01,  5.8154e-01,\n",
       "         -1.1578e+00, -1.1636e+00, -6.2485e-01, -1.2673e+00, -2.3580e+00,\n",
       "         -3.2042e+00, -1.5427e+00, -3.5132e+00, -1.2788e+00,  2.9866e-01,\n",
       "         -1.4991e+00,  4.0691e-01,  6.1131e-01,  2.3111e+00, -3.3196e-01,\n",
       "         -1.1993e-03,  1.0957e+00,  9.0917e-01, -1.2607e+00, -1.0453e+00,\n",
       "         -3.9043e-02,  2.0615e-01, -9.0536e-01, -2.5686e-01,  1.2777e+00,\n",
       "         -1.8452e-01,  2.9272e+00, -1.2399e-01,  1.8808e+00,  2.0655e-01,\n",
       "         -5.5985e-02,  2.2073e+00, -9.0045e-01,  7.0508e-01,  3.6678e-01,\n",
       "         -1.0719e+00,  5.7798e-01,  2.1031e+00,  2.2540e+00,  2.4927e+00,\n",
       "          1.6907e-01, -1.9596e+00,  1.2449e+00,  1.6488e+00,  2.9892e-02,\n",
       "         -8.2723e-02, -1.3266e+00,  3.8761e+00,  6.8010e-01,  1.0417e+00,\n",
       "         -7.5819e-01,  7.1905e-01, -1.9939e+00, -5.3829e-01,  1.3816e-01,\n",
       "          1.8468e+00,  3.3454e+00,  8.3303e-01,  3.7907e+00,  2.2379e+00,\n",
       "         -1.5533e+00,  1.6138e+00, -1.7580e-02,  2.5611e+00, -2.2759e+00,\n",
       "          2.0590e+00, -8.3571e-01, -2.4679e+00,  1.1069e+00,  1.6654e+00,\n",
       "          8.4313e-01, -7.7903e-02, -1.3359e+00, -6.5480e-01, -4.1178e-01,\n",
       "         -1.7150e+00,  3.8575e-01,  3.7139e-01, -5.6561e-02, -2.6551e+00,\n",
       "         -4.2824e-01,  7.2805e-01,  1.2579e+00,  3.2830e-01,  6.8553e-01,\n",
       "          9.1926e-01,  1.2243e+00,  1.1053e+00, -1.7860e-01,  1.9520e-01,\n",
       "          2.1318e+00, -8.5299e-01, -1.7641e+00,  8.5685e-01,  4.4391e-01,\n",
       "          1.8880e+00,  6.5907e-01, -3.3990e-01, -2.7573e+00,  1.1823e+00,\n",
       "         -1.0754e-01,  1.2089e+00, -2.4273e+00,  5.0654e-01,  2.1573e+00,\n",
       "         -1.2563e-01,  3.7208e-01, -1.9380e+00, -6.2419e-01,  1.9923e+00,\n",
       "         -1.1064e+00, -1.1098e+00, -1.5078e+00, -8.1407e-02,  1.0153e+00,\n",
       "         -1.2413e+00,  9.4589e-01,  5.2707e-01, -5.8105e-02,  1.5179e+00,\n",
       "          2.0354e+00, -2.0273e+00, -2.9937e+00,  2.0723e+00,  6.1526e-01,\n",
       "         -1.9616e+00,  2.7798e-01,  1.3736e+00,  1.4753e+00, -1.0830e+00,\n",
       "         -2.2124e+00,  1.4622e+00, -6.2176e-01,  4.6344e-01, -8.7029e-01],\n",
       "        [ 2.9541e+00,  8.9419e-01,  9.4200e-01, -2.1993e+00,  9.3675e-01,\n",
       "          1.1904e+00, -3.0785e+00,  1.7081e-01, -1.4640e+00, -2.0546e+00,\n",
       "         -9.4202e-01, -8.5465e-01,  1.4076e-01, -4.4971e-01, -2.4915e+00,\n",
       "         -2.6640e+00,  4.4406e+00, -3.0066e-01,  3.5306e-01, -6.9851e-01,\n",
       "         -5.5605e-01, -1.1432e-02,  9.3070e-01,  3.2520e+00,  4.1550e-01,\n",
       "          5.1312e-01,  2.6017e-01, -6.5921e-03,  1.9768e-01,  7.4467e-01,\n",
       "         -6.7432e-01, -2.8079e+00, -8.4727e-01, -1.2304e+00, -1.5296e+00,\n",
       "          2.4864e+00,  1.1744e+00,  1.0612e+00, -2.1313e+00, -2.3122e+00,\n",
       "          6.5321e+00, -1.7254e+00, -1.0730e+00, -5.9039e-01,  2.0944e+00,\n",
       "         -3.6730e-01,  5.3303e-01, -2.5497e+00,  1.3613e+00,  1.0639e+00,\n",
       "         -1.0243e+00, -4.4254e-01, -1.8101e+00, -2.9143e+00, -1.8046e-02,\n",
       "         -1.7491e+00, -1.4008e+00, -6.5927e-01, -6.8181e-01, -2.4849e+00,\n",
       "          3.8867e-01, -3.4219e-01,  1.3226e-01,  7.4792e-01, -2.8924e-01,\n",
       "         -6.9116e-01, -1.0175e+00,  1.5389e+00,  1.6403e+00,  2.2024e+00,\n",
       "         -1.5171e+00, -1.1485e+00, -6.0704e-01, -1.0776e+00, -8.8420e-01,\n",
       "         -1.9197e+00,  2.5590e+00, -6.2267e-01, -1.2890e+00,  1.6859e+00,\n",
       "         -1.0476e+00,  1.0485e+00, -7.6571e-01,  2.2952e-01,  1.8100e+00,\n",
       "          1.7638e+00, -2.9806e+00,  2.4850e-02, -2.0901e+00, -1.9935e+00,\n",
       "         -1.1251e+00, -1.2462e-01, -7.5072e-01,  1.4018e+00, -5.7960e-01,\n",
       "          7.3914e-01,  1.1315e+00,  1.3047e+00,  3.8122e+00,  4.6448e-01,\n",
       "         -7.9385e-01, -2.8117e-01, -2.1903e+00, -1.8042e+00, -9.8039e-01,\n",
       "         -1.4129e+00,  1.5775e+00,  1.3420e+00, -1.4413e+00, -6.3485e-01,\n",
       "         -1.1254e+00,  2.4943e+00, -1.7134e+00,  2.2930e-01,  5.7798e-01,\n",
       "         -1.6671e+00, -5.1925e-01, -1.4465e-01, -3.4934e+00,  1.2704e+00,\n",
       "          2.0863e+00,  8.8622e-01, -6.5919e-01,  2.3323e+00,  2.0074e-02,\n",
       "         -1.8307e-01,  2.3938e+00,  5.5347e+00,  1.2044e+00, -2.3177e-01,\n",
       "          2.9362e+00, -1.1296e+00, -6.0069e-01,  4.1283e-01,  7.4909e-01,\n",
       "          7.0912e-02, -2.3809e+00, -9.5762e-01,  6.0113e-01, -5.6482e-01,\n",
       "          4.0954e-01, -8.5508e-01, -2.2120e+00,  6.9018e-01, -2.2192e+00,\n",
       "         -8.2605e-01, -1.1422e+00, -9.6264e-01, -2.3156e-01,  1.3952e+00,\n",
       "          1.1100e+00,  3.2005e-01, -8.2877e-01,  8.6423e-01,  2.2669e+00,\n",
       "         -3.9475e-01, -1.0766e+00,  1.9241e+00, -3.0060e+00,  1.1786e+00,\n",
       "         -1.1391e+00, -2.1580e+00,  1.5640e+00, -4.2753e-01,  2.9735e+00,\n",
       "          1.3542e+00, -4.8040e-01,  3.9445e-01,  3.8106e-01,  1.0370e+00,\n",
       "         -1.5564e-01,  4.7205e-02, -2.0710e+00, -1.8733e+00, -1.8837e+00,\n",
       "          4.2750e+00, -2.0243e+00,  1.5785e+00,  1.0037e+00, -7.1812e-01,\n",
       "          1.5088e+00, -2.2813e+00, -2.1232e+00, -7.6310e-02, -2.3405e+00,\n",
       "          4.2797e-01, -1.9409e-01,  2.0274e+00,  6.1709e-01,  3.8879e-01,\n",
       "         -1.4562e+00, -3.6729e-01, -4.7892e-01,  5.3990e-01,  1.2157e-01,\n",
       "          2.5966e+00,  4.5364e-01,  5.6765e-01, -1.7083e+00, -5.2697e-01,\n",
       "         -1.1584e+00, -1.4184e-01, -7.5934e-01,  1.2380e+00, -3.6906e+00,\n",
       "          1.1029e+00,  5.1997e+00, -6.5581e-01, -1.4682e+00, -2.0225e+00,\n",
       "          6.0243e-01, -2.9093e-01, -1.1167e+00,  3.6189e-01,  1.3283e-01,\n",
       "         -2.7225e+00, -2.6316e-01,  2.6717e-01, -2.6152e+00,  2.2696e+00,\n",
       "          1.0343e+00,  2.3549e-01,  3.3981e+00,  9.9171e-01, -1.6602e+00,\n",
       "          1.6268e+00, -1.8121e+00, -1.7305e+00, -4.4296e-02, -4.6833e-02,\n",
       "          3.7438e-01, -1.2510e+00,  1.7457e+00,  4.4572e-02,  7.7436e-01,\n",
       "          4.1020e-01,  1.1180e+00, -1.6478e+00, -1.8674e+00,  2.0775e+00,\n",
       "          7.4338e-01,  6.5418e-01, -1.6719e+00, -2.3900e-01,  1.6313e+00,\n",
       "         -3.5230e-01, -1.8140e+00,  1.9528e+00,  1.4424e+00,  7.9795e-01,\n",
       "          1.3485e+00, -3.4777e+00,  1.5221e+00, -2.1085e+00, -2.5402e+00,\n",
       "         -3.2192e-01,  7.9548e-01, -2.3002e+00,  1.4106e+00, -1.9408e+00,\n",
       "          2.4318e-01,  1.7439e+00,  1.4268e+00,  4.2963e-01,  5.8055e-01,\n",
       "         -1.0738e+00, -1.2130e+00, -7.0070e-01, -1.2474e+00, -2.3632e+00,\n",
       "         -3.2803e+00, -1.5213e+00, -3.5163e+00, -1.3018e+00,  1.9663e-01,\n",
       "         -1.6918e+00,  4.9130e-01,  5.9688e-01,  2.2085e+00, -5.0718e-01,\n",
       "          3.9582e-03,  1.2086e+00,  9.3744e-01, -1.1392e+00, -1.1282e+00,\n",
       "         -1.6442e-01,  1.8737e-01, -1.0417e+00, -3.7412e-01,  1.3396e+00,\n",
       "         -2.7768e-01,  2.7765e+00, -1.0853e-01,  1.8914e+00,  2.7737e-01,\n",
       "         -1.6309e-01,  1.9929e+00, -9.3507e-01,  7.3654e-01,  4.4818e-01,\n",
       "         -9.2082e-01,  4.7597e-01,  2.2831e+00,  2.2085e+00,  2.8127e+00,\n",
       "         -8.2730e-02, -1.8990e+00,  1.4610e+00,  1.7488e+00,  1.1028e-01,\n",
       "         -1.1585e-01, -1.5522e+00,  4.1406e+00,  6.5164e-01,  1.0098e+00,\n",
       "         -6.3403e-01,  8.0987e-01, -1.8879e+00, -7.4644e-01,  1.3014e-01,\n",
       "          1.6540e+00,  3.2600e+00,  1.0291e+00,  4.1064e+00,  2.2797e+00,\n",
       "         -1.5593e+00,  1.6536e+00,  2.7582e-03,  2.5880e+00, -2.1385e+00,\n",
       "          2.0002e+00, -9.1431e-01, -2.4731e+00,  9.5184e-01,  1.6431e+00,\n",
       "          8.5777e-01,  1.4815e-02, -1.2340e+00, -6.3028e-01, -4.0082e-01,\n",
       "         -1.6225e+00,  3.7791e-01,  3.9153e-01, -7.8107e-02, -2.5737e+00,\n",
       "         -3.3056e-01,  7.5577e-01,  1.3454e+00,  4.7623e-01,  6.2973e-01,\n",
       "          8.7043e-01,  1.2486e+00,  1.2420e+00, -1.3326e-01,  1.0966e-01,\n",
       "          2.0087e+00, -7.0159e-01, -1.9317e+00,  9.3539e-01,  2.8877e-01,\n",
       "          1.9011e+00,  4.8296e-01, -4.1522e-01, -3.0353e+00,  1.4802e+00,\n",
       "         -2.3664e-01,  1.3144e+00, -2.5106e+00,  6.7418e-01,  1.9533e+00,\n",
       "          9.0326e-02,  3.0219e-01, -1.8506e+00, -5.8680e-01,  1.8328e+00,\n",
       "         -1.2665e+00, -1.0470e+00, -1.3527e+00, -3.1174e-01,  1.1602e+00,\n",
       "         -1.0644e+00,  1.0591e+00,  5.3072e-01, -1.4137e-02,  1.5169e+00,\n",
       "          2.1625e+00, -2.2517e+00, -3.0075e+00,  2.0654e+00,  6.0024e-01,\n",
       "         -1.8985e+00,  3.4548e-01,  1.4115e+00,  1.5323e+00, -1.0913e+00,\n",
       "         -2.3059e+00,  1.3456e+00, -5.9066e-01,  5.8633e-01, -7.3775e-01]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesformer_model(\n",
    "    torch.rand((2, 8, 3, 224, 224)) # n t c h w\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeSFormerClassifier(nn.Module):\n",
    "#     def __init__(self, num_classes: int):\n",
    "#         super(TimeSFormerClassifier, self).__init__()\n",
    "        \n",
    "#         self.timesformer = TimesformerModel.from_pretrained(\n",
    "#             pretrained_model_name_or_path=\"facebook/timesformer-base-finetuned-k400\",\n",
    "#             cache_dir=\"/Users/artemmerinov/data/backbones/\",\n",
    "#         )\n",
    "#         hidden_size = self.timesformer.config.hidden_size\n",
    "        \n",
    "#         # Classifier head\n",
    "#         self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         outputs = self.timesformer(pixel_values=x)\n",
    "#         h_cls = outputs.last_hidden_state[:, 0]\n",
    "#         logits = self.classifier(h_cls).logits\n",
    "#         probs = F.softmax(logits, dim=1)\n",
    "#         return probs\n",
    "\n",
    "# timesformer(\n",
    "#     torch.rand((2, 8, 3, 224, 224)) # n t c h w\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in model.named_parameters():\n",
    "#     print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "vit_model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224', \n",
    "    cache_dir=\"/Users/artemmerinov/data/backbones/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesformer_model.load_state_dict(vit_model.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.timesformer import Timesformer, timesformer_base_patch16_224\n",
    "from timm.models import vision_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######USING ATTENTION STYLE:  divided\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timesformer(\n",
       "  (patch_embed): VideoPatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x SpaceTimeBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): VarAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (timeattn): VarAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm3): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Linear(in_features=768, out_features=2000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesformer = timesformer_base_patch16_224(pretrained=True, time_init='zeros', num_frames=8, num_classes=2000)\n",
    "timesformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2057, -0.6189, -1.0965,  ...,  0.5336, -0.0452,  0.0475],\n",
       "        [-0.2364, -0.5886, -1.0905,  ...,  0.4850, -0.0633,  0.0953]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesformer(\n",
    "    torch.rand((2, 8, 3, 224, 224)) # n t c h w\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "######USING ATTENTION STYLE:  divided\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.opts.opts import parser\n",
    "from src.utils.reproducibility import make_reproducible\n",
    "from src.models.model import VideoModel\n",
    "from src.dataset.video_dataset import VideoDataset\n",
    "from src.dataset.video_dataset import prepare_clips_data\n",
    "from src.dataset.video_transforms import (\n",
    "    IdentityTransform,\n",
    "    GroupScale, \n",
    "    GroupCenterCrop, \n",
    "    GroupRandomCrop,\n",
    "    GroupMultiScaleCrop,\n",
    "    Stack, \n",
    "    ToTorchFormatTensor, \n",
    "    GroupNormalize, \n",
    "    GroupRandomHorizontalFlip\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VideoModel(\n",
    "    num_classes=222, \n",
    "    num_segments=8, \n",
    "    base_model=\"TimeSformer\",\n",
    "    fusion_mode=None,\n",
    "    dropout=0.4,\n",
    "    verbose=False,\n",
    ").to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4901e-02, -7.2001e-02,  1.4783e-02, -1.5669e-02,  3.1846e-02,\n",
       "          5.3027e-02, -3.8594e-02, -1.2508e-02,  4.2211e-04, -2.4625e-02,\n",
       "         -4.6277e-02, -7.3088e-03, -3.8025e-02, -6.9849e-03,  4.5247e-02,\n",
       "          2.1998e-02, -6.3838e-03,  4.4167e-02, -2.4993e-02, -1.5703e-02,\n",
       "          2.8893e-02,  2.9232e-02,  2.7696e-02,  5.0412e-03,  6.9871e-03,\n",
       "          7.6024e-02,  3.1426e-02,  4.8192e-02, -4.2997e-02, -2.6138e-02,\n",
       "         -9.0722e-03,  4.6863e-03,  4.0819e-02,  4.6069e-02,  5.8568e-03,\n",
       "         -8.4892e-02,  2.5731e-03,  8.4424e-02, -3.9140e-02, -1.8981e-03,\n",
       "         -1.4111e-02,  1.4155e-02,  9.6834e-03, -2.6602e-02, -4.5974e-02,\n",
       "          4.3976e-02,  7.0189e-02,  4.6026e-02,  5.6329e-02, -3.0303e-02,\n",
       "          9.2729e-04, -7.1195e-02,  7.4970e-02,  4.9899e-02,  9.2358e-03,\n",
       "         -3.2531e-02, -7.5771e-02,  2.5686e-02,  2.4376e-02, -7.4603e-03,\n",
       "         -3.4853e-02,  2.5763e-02,  2.0924e-02,  1.7369e-03, -3.5903e-02,\n",
       "          1.2515e-02,  3.7630e-03, -5.3458e-02,  2.3239e-02, -4.7504e-02,\n",
       "          1.0082e-01, -2.5602e-02, -1.2734e-03,  1.8372e-02,  4.1594e-02,\n",
       "          2.7826e-03,  5.5057e-02, -4.0191e-03, -9.3855e-02, -4.4248e-02,\n",
       "          4.0922e-03, -6.6145e-03, -1.9554e-02, -2.6286e-02, -3.2545e-02,\n",
       "         -2.8193e-02,  5.5621e-02,  1.7964e-04,  7.7360e-02,  2.6698e-02,\n",
       "         -1.7971e-02, -2.7759e-02, -5.3541e-02,  3.0733e-02, -4.5928e-02,\n",
       "          4.2330e-02,  6.5577e-02, -8.8415e-03, -3.9723e-03, -2.2260e-02,\n",
       "          7.3317e-03,  1.0716e-02, -3.7199e-02,  2.0482e-02,  5.4750e-02,\n",
       "         -1.1334e-02, -2.0011e-02, -3.8695e-02, -9.3289e-02, -2.8389e-02,\n",
       "          4.4133e-02, -3.4291e-02, -9.0823e-02,  9.2883e-02, -3.3185e-02,\n",
       "         -2.9274e-02,  3.3867e-02,  7.1631e-03,  6.9189e-03, -3.2365e-03,\n",
       "         -4.5598e-02,  7.6412e-02,  1.0543e-01,  8.1823e-03,  2.3837e-02,\n",
       "          3.1616e-02,  1.9504e-02,  5.8794e-02, -2.2307e-03,  5.2481e-02,\n",
       "          3.8444e-02, -1.1320e-02,  6.8969e-02, -1.8658e-02, -3.5053e-02,\n",
       "          2.1922e-02,  7.3282e-02,  8.0747e-02,  4.3010e-04, -1.8188e-02,\n",
       "         -7.9370e-03, -4.5666e-02, -7.9353e-03, -1.6801e-02,  7.9081e-03,\n",
       "          7.7920e-02, -1.5904e-02,  3.4567e-02,  4.1193e-02, -1.9853e-02,\n",
       "         -3.6572e-02,  8.2696e-03,  1.0604e-02, -2.5324e-02, -8.9529e-02,\n",
       "          8.3509e-03,  1.7376e-02,  1.2509e-02,  4.3888e-02, -2.1496e-02,\n",
       "         -3.8798e-02,  5.8545e-03, -6.4320e-02, -5.2559e-02,  4.1602e-02,\n",
       "          1.6699e-02,  3.2599e-02, -4.9230e-03, -3.7509e-02, -1.3700e-02,\n",
       "          4.1991e-03, -2.2806e-02, -5.3165e-02, -7.7562e-03, -2.0960e-02,\n",
       "          6.3824e-02,  1.0293e-02,  2.6241e-02,  3.7892e-02,  2.8937e-02,\n",
       "         -1.2210e-02, -1.3735e-02, -1.0043e-03,  1.7277e-02, -3.1868e-02,\n",
       "         -2.0792e-02, -6.3082e-02, -1.1314e-02, -3.9189e-02, -1.3784e-02,\n",
       "         -7.8771e-03, -5.4719e-02, -2.4370e-02, -3.3151e-02,  6.7099e-02,\n",
       "         -2.1042e-02, -6.7404e-02, -4.4032e-03, -1.3827e-02,  1.3322e-02,\n",
       "         -5.9151e-02, -5.8333e-04,  3.8193e-03,  3.3961e-02, -3.2309e-02,\n",
       "         -8.6248e-04,  1.9473e-02,  2.4677e-02, -3.4558e-02,  9.5782e-02,\n",
       "          2.1588e-02, -2.1751e-02, -8.5597e-02, -3.3697e-02,  3.4526e-02,\n",
       "         -3.5386e-02, -1.0359e-01,  2.6482e-03, -6.1165e-02, -9.8641e-03,\n",
       "         -6.0498e-02,  6.3568e-02],\n",
       "        [-8.7234e-03, -5.9234e-02,  1.2854e-02, -8.0680e-03,  1.8225e-03,\n",
       "          2.5842e-02, -2.6587e-03, -2.2556e-02,  1.3859e-02,  1.6986e-02,\n",
       "         -2.2562e-02,  1.4056e-02,  2.7035e-02,  9.1308e-03,  7.8814e-02,\n",
       "          6.9632e-03, -1.0085e-02,  1.3063e-02,  2.0947e-02, -9.7588e-03,\n",
       "          4.2659e-02,  1.5387e-02,  2.8705e-02, -3.1639e-02, -2.8861e-02,\n",
       "          4.8979e-02, -1.5174e-02,  2.2485e-02, -6.6607e-03, -3.9717e-02,\n",
       "          2.8767e-02, -3.2069e-02,  4.7265e-02,  4.7177e-02, -3.3638e-02,\n",
       "         -5.4096e-02,  1.4659e-02, -9.1144e-03, -4.3401e-02,  2.1729e-02,\n",
       "         -2.2023e-02,  2.8785e-02, -5.5535e-04, -1.7212e-02,  1.8926e-02,\n",
       "          2.3289e-02,  9.4929e-02,  4.6974e-02,  6.2451e-02, -1.3366e-02,\n",
       "         -3.0313e-03, -4.9118e-02,  3.8182e-02,  4.7012e-02,  2.9010e-02,\n",
       "         -3.5430e-02, -8.9828e-02,  8.4845e-03,  5.1607e-02, -1.5321e-02,\n",
       "         -1.1060e-01, -3.8191e-02,  2.0673e-02,  1.6029e-02, -2.0544e-02,\n",
       "          6.6124e-03, -1.0251e-02, -4.3156e-03,  3.0839e-02, -7.6368e-02,\n",
       "          6.7180e-02, -2.4372e-02, -3.7245e-02,  1.3336e-02, -1.1092e-03,\n",
       "         -4.0122e-02,  8.0142e-03, -6.0074e-03, -4.0449e-02,  4.9022e-02,\n",
       "         -4.7516e-02, -2.6189e-02, -8.6632e-03, -6.7770e-02, -1.1129e-02,\n",
       "         -1.9287e-02,  5.2532e-03, -1.8142e-02,  1.9214e-02, -1.0806e-02,\n",
       "         -3.1842e-02,  2.9166e-03, -8.3645e-03,  1.0381e-02, -7.1439e-03,\n",
       "          3.7455e-02,  5.4829e-02, -3.5280e-02,  6.9754e-03, -4.4782e-02,\n",
       "          1.2916e-02,  5.4547e-02, -4.8487e-02, -7.6417e-03,  3.4791e-02,\n",
       "         -6.0581e-03, -6.9966e-03,  3.6361e-02, -4.8991e-02,  3.8357e-02,\n",
       "          3.9778e-02, -3.1427e-02, -3.7807e-02,  1.0028e-01, -3.0891e-02,\n",
       "         -9.5582e-03,  1.2528e-02, -1.7189e-02, -5.4357e-02,  5.0726e-02,\n",
       "         -4.3765e-02,  2.0728e-02,  9.8275e-02,  1.9740e-02, -1.5349e-02,\n",
       "         -3.7825e-02,  2.8040e-02,  4.4630e-02, -6.0067e-03,  2.5276e-02,\n",
       "          4.0390e-03, -7.7884e-03,  2.8739e-02,  8.4678e-03,  1.0519e-02,\n",
       "         -3.3962e-03,  6.2631e-03,  8.9756e-02, -1.7271e-02,  1.0866e-02,\n",
       "          1.9556e-03, -2.1467e-02, -4.6675e-03, -2.9994e-03, -4.7621e-02,\n",
       "          8.2780e-02, -1.3143e-02,  1.5621e-02,  6.0942e-02, -4.0040e-02,\n",
       "         -2.6223e-02,  9.2164e-05,  2.5756e-02, -6.6992e-02, -6.2594e-02,\n",
       "          1.5669e-02,  2.5705e-02,  3.4724e-02,  4.1020e-02, -5.6545e-02,\n",
       "         -2.9684e-02, -1.0816e-03, -2.6829e-02, -1.0014e-01,  3.6897e-02,\n",
       "         -8.8143e-03,  6.5801e-02, -2.8217e-02,  4.9275e-03,  7.0095e-03,\n",
       "          4.0081e-02, -2.8816e-02, -7.0228e-02,  1.6822e-02,  3.0754e-02,\n",
       "          1.9759e-02, -1.4184e-02,  1.6909e-02,  6.9176e-02,  5.2792e-02,\n",
       "         -3.6593e-02, -5.6915e-03, -6.2053e-02,  4.3764e-02,  1.0734e-02,\n",
       "          4.0733e-03, -3.9492e-02,  8.3344e-03, -2.3190e-02, -2.6936e-02,\n",
       "          5.9074e-02, -4.5822e-02, -1.9807e-02, -7.6999e-02,  7.8059e-02,\n",
       "          4.4460e-02, -4.7313e-02,  7.3800e-03, -3.5944e-02,  5.2886e-02,\n",
       "         -3.4226e-02, -3.0727e-02, -5.3885e-02,  6.0421e-03,  2.0162e-02,\n",
       "          2.1251e-02,  3.0425e-02,  3.2794e-02, -3.4421e-02,  6.6832e-02,\n",
       "         -2.5603e-03,  5.1121e-03, -1.9887e-02,  9.1078e-03,  7.8433e-03,\n",
       "         -1.1698e-02, -9.1959e-02, -1.7108e-02, -4.9932e-02, -3.1222e-03,\n",
       "         -2.1737e-02,  4.5829e-03]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "    torch.rand((2, 8*3, 224, 224)) # n t c h w\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 videos in the list There are 13 videos as video files There are 0 videos that present in the list but are missing as videos.\n",
      "Number of clips: 507 for mode train\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "args.base_model = \"TimeSformer\"\n",
    "args.__dict__\n",
    "\n",
    "tr_clip_path_to_video_arr, tr_clip_start_arr, tr_clip_end_arr, tr_clip_action_id_arr, tr_clip_mistake_arr = prepare_clips_data(\n",
    "    raw_annotation_file=args.raw_annotation_file,\n",
    "    holoassist_dir=args.holoassist_dir, \n",
    "    split_dir=args.split_dir,\n",
    "    fine_grained_actions_map_file=args.fine_grained_actions_map_file,\n",
    "    mode=\"train\",\n",
    ")\n",
    "\n",
    "tr_transform = Compose([\n",
    "    GroupMultiScaleCrop(224, [1, .875]),\n",
    "    GroupRandomHorizontalFlip(),\n",
    "    Stack(roll=False),\n",
    "    ToTorchFormatTensor(div=(args.base_model not in ['BNInception'])),\n",
    "    GroupNormalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "tr_dataset = VideoDataset(\n",
    "    clip_path_to_video_arr=tr_clip_path_to_video_arr,\n",
    "    clip_start_arr=tr_clip_start_arr,\n",
    "    clip_end_arr=tr_clip_end_arr,\n",
    "    clip_label_arr=tr_clip_action_id_arr,\n",
    "    num_segments=args.num_segments,\n",
    "    transform=tr_transform,\n",
    "    mode=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = tr_dataset[123]\n",
    "y = Rearrange(\"(t c) h w -> 1 (t c) h w\", c=3, t=args.num_segments, h=224, w=224)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 224, 224])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Output 0 of ReshapeAliasBackward0 is a view and is being modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/src/models/model.py:166\u001b[0m, in \u001b[0;36mVideoModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m     x \u001b[38;5;241m=\u001b[39m Rearrange(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn (t c) h w -> n t c h w\u001b[39m\u001b[38;5;124m\"\u001b[39m, n\u001b[38;5;241m=\u001b[39mn, c\u001b[38;5;241m=\u001b[39mc, t\u001b[38;5;241m=\u001b[39mt, h\u001b[38;5;241m=\u001b[39mh, w\u001b[38;5;241m=\u001b[39mw)(x)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 166\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# (!) it is important to have (t c) and not (c t)\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/src/models/timesformer.py:352\u001b[0m, in \u001b[0;36mTimesformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 352\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/src/models/timesformer.py:342\u001b[0m, in \u001b[0;36mTimesformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    339\u001b[0m f \u001b[38;5;241m=\u001b[39m curr_frames\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 342\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinops_from_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinops_to_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinops_from_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinops_to_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    347\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_logits(x)\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/src/models/timesformer.py:167\u001b[0m, in \u001b[0;36mSpaceTimeBlock.forward\u001b[0;34m(self, x, einops_from_space, einops_to_space, einops_from_time, einops_to_time, time_n, space_f)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, einops_from_space, einops_to_space, einops_from_time, einops_to_time,\n\u001b[1;32m    165\u001b[0m             time_n, space_f):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_style \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdivided\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m         time_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meinops_from_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meinops_to_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m         time_residual \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m time_output\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;66;03m# time_residual_norm = self.norm1(time_residual)\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/src/models/timesformer.py:106\u001b[0m, in \u001b[0;36mVarAttention.forward\u001b[0;34m(self, x, einops_from, einops_to, **einops_dims)\u001b[0m\n\u001b[1;32m    103\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    104\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: rearrange(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n (h d) -> (b h) n d\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39mh), (q, k, v))\n\u001b[0;32m--> 106\u001b[0m \u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# splice out CLS token at index 1\u001b[39;00m\n\u001b[1;32m    109\u001b[0m (cls_q, q_), (cls_k, k_), (cls_v, v_) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: (t[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m], t[:, \u001b[38;5;241m1\u001b[39m:]), (q, k, v))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Output 0 of ReshapeAliasBackward0 is a view and is being modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one."
     ]
    }
   ],
   "source": [
    "model(\n",
    "    torch.rand((1, 8*3, 224, 224))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type vit to instantiate a model of type timesformer. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimesformerForVideoClassification\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTimesformerForVideoClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/vit-base-patch16-224\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/artemmerinov/data/backbones\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3677\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3669\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3670\u001b[0m     (\n\u001b[1;32m   3671\u001b[0m         model,\n\u001b[1;32m   3672\u001b[0m         missing_keys,\n\u001b[1;32m   3673\u001b[0m         unexpected_keys,\n\u001b[1;32m   3674\u001b[0m         mismatched_keys,\n\u001b[1;32m   3675\u001b[0m         offload_index,\n\u001b[1;32m   3676\u001b[0m         error_msgs,\n\u001b[0;32m-> 3677\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3684\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3685\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3688\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3689\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3696\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/PycharmProjects/holoassist/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3971\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3969\u001b[0m base_model_expected_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model_to_load\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   3970\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m expected_keys_not_prefixed \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m base_model_expected_keys \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m loaded_keys):\n\u001b[0;32m-> 3971\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3972\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe state dictionary of the model you are trying to load is corrupted. Are you sure it was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3973\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperly saved?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3974\u001b[0m     )\n\u001b[1;32m   3975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3976\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mValueError\u001b[0m: The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?"
     ]
    }
   ],
   "source": [
    "from transformers import TimesformerForVideoClassification\n",
    "\n",
    "model = TimesformerForVideoClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"google/vit-base-patch16-224\",\n",
    "    cache_dir=\"/Users/artemmerinov/data/backbones\",\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
