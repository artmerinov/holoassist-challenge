{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from src.opts.opts import parser\n",
    "from src.utils.reproducibility import make_reproducible\n",
    "from src.utils.model_specs import model_size\n",
    "from src.models.model import VideoModel\n",
    "from src.dataset.video_dataset import VideoDataset, prepare_clips_data\n",
    "from src.dataset.video_transforms import (\n",
    "    GroupMultiScaleCrop, Stack, ToTorchFormatTensor, GroupNormalize,\n",
    ")\n",
    "from src.utils.meters import AverageMeter\n",
    "from src.utils.metrics import calc_accuracy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making reproducible on seed 0\n",
      "Namespace(holoassist_dir='/Users/artemmerinov/data/holoassist', raw_annotation_file='/Users/artemmerinov/data/holoassist/data-annotation-trainval-v1_1.json', split_dir='/Users/artemmerinov/data/holoassist/data-splits-v1', fine_grained_actions_map_file='/Users/artemmerinov/data/holoassist/fine_grained_actions_map.txt', dataset_name='holoassist', fusion_mode=None, base_model='HORST', num_segments=8, dropout=0.5, resume=None, start_epoch=0, num_epochs=10, lr=0.01, momentum=0.9, weight_decay=0.0005, clip_gradient=None, checkpoint_interval=3, runs_path='runs/', batch_size=16, num_workers=4, prefetch_factor=2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artemmerinov/PycharmProjects/holoassist-challenge/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/artemmerinov/PycharmProjects/holoassist-challenge/.venv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name swin_base_patch4_window7_224_in22k to current swin_base_patch4_window7_224.ms_in22k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#params: 6144 base_model.backbone_model.model.patch_embed.proj.weight\n",
      "#params: 128 base_model.backbone_model.model.patch_embed.proj.bias\n",
      "#params: 128 base_model.backbone_model.model.patch_embed.norm.weight\n",
      "#params: 128 base_model.backbone_model.model.patch_embed.norm.bias\n",
      "#params: 128 base_model.backbone_model.model.layers.0.blocks.0.norm1.weight\n",
      "#params: 128 base_model.backbone_model.model.layers.0.blocks.0.norm1.bias\n",
      "#params: 676 base_model.backbone_model.model.layers.0.blocks.0.attn.relative_position_bias_table\n",
      "#params: 49152 base_model.backbone_model.model.layers.0.blocks.0.attn.qkv.weight\n",
      "#params: 384 base_model.backbone_model.model.layers.0.blocks.0.attn.qkv.bias\n",
      "#params: 16384 base_model.backbone_model.model.layers.0.blocks.0.attn.proj.weight\n",
      "#params: 128 base_model.backbone_model.model.layers.0.blocks.0.attn.proj.bias\n",
      "#params: 128 base_model.backbone_model.model.layers.0.blocks.0.norm2.weight\n",
      "#params: 128 base_model.backbone_model.model.layers.0.blocks.0.norm2.bias\n",
      "#params: 65536 base_model.backbone_model.model.layers.0.blocks.0.mlp.fc1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.0.blocks.0.mlp.fc1.bias\n",
      "#params: 65536 base_model.backbone_model.model.layers.0.blocks.0.mlp.fc2.weight\n",
      "#params: 128 base_model.backbone_model.model.layers.0.blocks.0.mlp.fc2.bias\n",
      "#params: 128 base_model.backbone_model.model.layers.0.blocks.1.norm1.weight\n",
      "#params: 128 base_model.backbone_model.model.layers.0.blocks.1.norm1.bias\n",
      "#params: 676 base_model.backbone_model.model.layers.0.blocks.1.attn.relative_position_bias_table\n",
      "#params: 49152 base_model.backbone_model.model.layers.0.blocks.1.attn.qkv.weight\n",
      "#params: 384 base_model.backbone_model.model.layers.0.blocks.1.attn.qkv.bias\n",
      "#params: 16384 base_model.backbone_model.model.layers.0.blocks.1.attn.proj.weight\n",
      "#params: 128 base_model.backbone_model.model.layers.0.blocks.1.attn.proj.bias\n",
      "#params: 128 base_model.backbone_model.model.layers.0.blocks.1.norm2.weight\n",
      "#params: 128 base_model.backbone_model.model.layers.0.blocks.1.norm2.bias\n",
      "#params: 65536 base_model.backbone_model.model.layers.0.blocks.1.mlp.fc1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.0.blocks.1.mlp.fc1.bias\n",
      "#params: 65536 base_model.backbone_model.model.layers.0.blocks.1.mlp.fc2.weight\n",
      "#params: 128 base_model.backbone_model.model.layers.0.blocks.1.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.1.downsample.norm.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.1.downsample.norm.bias\n",
      "#params: 131072 base_model.backbone_model.model.layers.1.downsample.reduction.weight\n",
      "#params: 256 base_model.backbone_model.model.layers.1.blocks.0.norm1.weight\n",
      "#params: 256 base_model.backbone_model.model.layers.1.blocks.0.norm1.bias\n",
      "#params: 1352 base_model.backbone_model.model.layers.1.blocks.0.attn.relative_position_bias_table\n",
      "#params: 196608 base_model.backbone_model.model.layers.1.blocks.0.attn.qkv.weight\n",
      "#params: 768 base_model.backbone_model.model.layers.1.blocks.0.attn.qkv.bias\n",
      "#params: 65536 base_model.backbone_model.model.layers.1.blocks.0.attn.proj.weight\n",
      "#params: 256 base_model.backbone_model.model.layers.1.blocks.0.attn.proj.bias\n",
      "#params: 256 base_model.backbone_model.model.layers.1.blocks.0.norm2.weight\n",
      "#params: 256 base_model.backbone_model.model.layers.1.blocks.0.norm2.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.1.blocks.0.mlp.fc1.weight\n",
      "#params: 1024 base_model.backbone_model.model.layers.1.blocks.0.mlp.fc1.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.1.blocks.0.mlp.fc2.weight\n",
      "#params: 256 base_model.backbone_model.model.layers.1.blocks.0.mlp.fc2.bias\n",
      "#params: 256 base_model.backbone_model.model.layers.1.blocks.1.norm1.weight\n",
      "#params: 256 base_model.backbone_model.model.layers.1.blocks.1.norm1.bias\n",
      "#params: 1352 base_model.backbone_model.model.layers.1.blocks.1.attn.relative_position_bias_table\n",
      "#params: 196608 base_model.backbone_model.model.layers.1.blocks.1.attn.qkv.weight\n",
      "#params: 768 base_model.backbone_model.model.layers.1.blocks.1.attn.qkv.bias\n",
      "#params: 65536 base_model.backbone_model.model.layers.1.blocks.1.attn.proj.weight\n",
      "#params: 256 base_model.backbone_model.model.layers.1.blocks.1.attn.proj.bias\n",
      "#params: 256 base_model.backbone_model.model.layers.1.blocks.1.norm2.weight\n",
      "#params: 256 base_model.backbone_model.model.layers.1.blocks.1.norm2.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.1.blocks.1.mlp.fc1.weight\n",
      "#params: 1024 base_model.backbone_model.model.layers.1.blocks.1.mlp.fc1.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.1.blocks.1.mlp.fc2.weight\n",
      "#params: 256 base_model.backbone_model.model.layers.1.blocks.1.mlp.fc2.bias\n",
      "#params: 1024 base_model.backbone_model.model.layers.2.downsample.norm.weight\n",
      "#params: 1024 base_model.backbone_model.model.layers.2.downsample.norm.bias\n",
      "#params: 524288 base_model.backbone_model.model.layers.2.downsample.reduction.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.0.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.0.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.0.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.0.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.0.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.0.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.0.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.0.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.0.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.0.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.0.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.0.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.0.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.1.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.1.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.1.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.1.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.1.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.1.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.1.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.1.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.1.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.1.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.1.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.1.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.1.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.2.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.2.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.2.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.2.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.2.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.2.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.2.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.2.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.2.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.2.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.2.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.2.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.2.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.3.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.3.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.3.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.3.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.3.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.3.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.3.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.3.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.3.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.3.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.3.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.3.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.3.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.4.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.4.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.4.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.4.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.4.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.4.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.4.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.4.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.4.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.4.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.4.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.4.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.4.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.5.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.5.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.5.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.5.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.5.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.5.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.5.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.5.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.5.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.5.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.5.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.5.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.5.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.6.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.6.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.6.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.6.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.6.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.6.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.6.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.6.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.6.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.6.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.6.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.6.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.6.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.7.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.7.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.7.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.7.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.7.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.7.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.7.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.7.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.7.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.7.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.7.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.7.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.7.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.8.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.8.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.8.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.8.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.8.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.8.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.8.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.8.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.8.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.8.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.8.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.8.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.8.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.9.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.9.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.9.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.9.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.9.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.9.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.9.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.9.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.9.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.9.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.9.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.9.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.9.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.10.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.10.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.10.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.10.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.10.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.10.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.10.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.10.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.10.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.10.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.10.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.10.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.10.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.11.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.11.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.11.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.11.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.11.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.11.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.11.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.11.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.11.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.11.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.11.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.11.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.11.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.12.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.12.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.12.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.12.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.12.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.12.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.12.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.12.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.12.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.12.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.12.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.12.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.12.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.13.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.13.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.13.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.13.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.13.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.13.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.13.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.13.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.13.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.13.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.13.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.13.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.13.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.14.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.14.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.14.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.14.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.14.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.14.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.14.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.14.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.14.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.14.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.14.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.14.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.14.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.15.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.15.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.15.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.15.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.15.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.15.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.15.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.15.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.15.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.15.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.15.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.15.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.15.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.16.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.16.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.16.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.16.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.16.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.16.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.16.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.16.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.16.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.16.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.16.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.16.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.16.mlp.fc2.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.17.norm1.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.17.norm1.bias\n",
      "#params: 2704 base_model.backbone_model.model.layers.2.blocks.17.attn.relative_position_bias_table\n",
      "#params: 786432 base_model.backbone_model.model.layers.2.blocks.17.attn.qkv.weight\n",
      "#params: 1536 base_model.backbone_model.model.layers.2.blocks.17.attn.qkv.bias\n",
      "#params: 262144 base_model.backbone_model.model.layers.2.blocks.17.attn.proj.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.17.attn.proj.bias\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.17.norm2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.17.norm2.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.17.mlp.fc1.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.2.blocks.17.mlp.fc1.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.2.blocks.17.mlp.fc2.weight\n",
      "#params: 512 base_model.backbone_model.model.layers.2.blocks.17.mlp.fc2.bias\n",
      "#params: 2048 base_model.backbone_model.model.layers.3.downsample.norm.weight\n",
      "#params: 2048 base_model.backbone_model.model.layers.3.downsample.norm.bias\n",
      "#params: 2097152 base_model.backbone_model.model.layers.3.downsample.reduction.weight\n",
      "#params: 1024 base_model.backbone_model.model.layers.3.blocks.0.norm1.weight\n",
      "#params: 1024 base_model.backbone_model.model.layers.3.blocks.0.norm1.bias\n",
      "#params: 5408 base_model.backbone_model.model.layers.3.blocks.0.attn.relative_position_bias_table\n",
      "#params: 3145728 base_model.backbone_model.model.layers.3.blocks.0.attn.qkv.weight\n",
      "#params: 3072 base_model.backbone_model.model.layers.3.blocks.0.attn.qkv.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.3.blocks.0.attn.proj.weight\n",
      "#params: 1024 base_model.backbone_model.model.layers.3.blocks.0.attn.proj.bias\n",
      "#params: 1024 base_model.backbone_model.model.layers.3.blocks.0.norm2.weight\n",
      "#params: 1024 base_model.backbone_model.model.layers.3.blocks.0.norm2.bias\n",
      "#params: 4194304 base_model.backbone_model.model.layers.3.blocks.0.mlp.fc1.weight\n",
      "#params: 4096 base_model.backbone_model.model.layers.3.blocks.0.mlp.fc1.bias\n",
      "#params: 4194304 base_model.backbone_model.model.layers.3.blocks.0.mlp.fc2.weight\n",
      "#params: 1024 base_model.backbone_model.model.layers.3.blocks.0.mlp.fc2.bias\n",
      "#params: 1024 base_model.backbone_model.model.layers.3.blocks.1.norm1.weight\n",
      "#params: 1024 base_model.backbone_model.model.layers.3.blocks.1.norm1.bias\n",
      "#params: 5408 base_model.backbone_model.model.layers.3.blocks.1.attn.relative_position_bias_table\n",
      "#params: 3145728 base_model.backbone_model.model.layers.3.blocks.1.attn.qkv.weight\n",
      "#params: 3072 base_model.backbone_model.model.layers.3.blocks.1.attn.qkv.bias\n",
      "#params: 1048576 base_model.backbone_model.model.layers.3.blocks.1.attn.proj.weight\n",
      "#params: 1024 base_model.backbone_model.model.layers.3.blocks.1.attn.proj.bias\n",
      "#params: 1024 base_model.backbone_model.model.layers.3.blocks.1.norm2.weight\n",
      "#params: 1024 base_model.backbone_model.model.layers.3.blocks.1.norm2.bias\n",
      "#params: 4194304 base_model.backbone_model.model.layers.3.blocks.1.mlp.fc1.weight\n",
      "#params: 4096 base_model.backbone_model.model.layers.3.blocks.1.mlp.fc1.bias\n",
      "#params: 4194304 base_model.backbone_model.model.layers.3.blocks.1.mlp.fc2.weight\n",
      "#params: 1024 base_model.backbone_model.model.layers.3.blocks.1.mlp.fc2.bias\n",
      "#params: 1024 base_model.backbone_model.model.norm.weight\n",
      "#params: 1024 base_model.backbone_model.model.norm.bias\n",
      "#params: 22365184 base_model.backbone_model.model.head.fc.weight\n",
      "#params: 21841 base_model.backbone_model.model.head.fc.bias\n",
      "#params: 1024 base_model.norm.weight\n",
      "#params: 1024 base_model.norm.bias\n",
      "#params: 9437184 base_model.recurrent_model.0.layers.b0l0.encoder.0.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b0l0.encoder.1.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b0l0.encoder.1.bias\n",
      "#params: 18874368 base_model.recurrent_model.0.layers.b0l0.out_t.0.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b0l0.out_t.1.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b0l0.out_t.1.bias\n",
      "#params: 262144 base_model.recurrent_model.0.layers.b0l0.out_t.3.f.1.weight\n",
      "#params: 256 base_model.recurrent_model.0.layers.b0l0.out_t.3.f.1.bias\n",
      "#params: 262144 base_model.recurrent_model.0.layers.b0l0.out_t.3.f.3.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b0l0.out_t.3.f.3.bias\n",
      "#params: 9437184 base_model.recurrent_model.0.layers.b0l0.mha.q_pre_f.0.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b0l0.mha.q_pre_f.1.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b0l0.mha.q_pre_f.1.bias\n",
      "#params: 18 base_model.recurrent_model.0.layers.b0l0.mha.q_sa.f.0.weight\n",
      "#params: 1 base_model.recurrent_model.0.layers.b0l0.mha.q_sa.f.0.bias\n",
      "#params: 18874368 base_model.recurrent_model.0.layers.b0l0.mha.k_pre_f.0.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b0l0.mha.k_pre_f.1.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b0l0.mha.k_pre_f.1.bias\n",
      "#params: 18 base_model.recurrent_model.0.layers.b0l0.mha.k_sa.f.0.weight\n",
      "#params: 1 base_model.recurrent_model.0.layers.b0l0.mha.k_sa.f.0.bias\n",
      "#params: 18874368 base_model.recurrent_model.0.layers.b0l0.mha.v_pre_f.0.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b0l0.mha.v_pre_f.1.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b0l0.mha.v_pre_f.1.bias\n",
      "#params: 18 base_model.recurrent_model.0.layers.b0l0.mha.v_sa.f.0.weight\n",
      "#params: 1 base_model.recurrent_model.0.layers.b0l0.mha.v_sa.f.0.bias\n",
      "#params: 1048576 base_model.recurrent_model.0.layers.b0l0.mha.v_post_f.0.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b0l0.mha.v_post_f.1.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b0l0.mha.v_post_f.1.bias\n",
      "#params: 9437184 base_model.recurrent_model.0.layers.b1l0.encoder.0.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b1l0.encoder.1.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b1l0.encoder.1.bias\n",
      "#params: 18874368 base_model.recurrent_model.0.layers.b1l0.out_t.0.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b1l0.out_t.1.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b1l0.out_t.1.bias\n",
      "#params: 262144 base_model.recurrent_model.0.layers.b1l0.out_t.3.f.1.weight\n",
      "#params: 256 base_model.recurrent_model.0.layers.b1l0.out_t.3.f.1.bias\n",
      "#params: 262144 base_model.recurrent_model.0.layers.b1l0.out_t.3.f.3.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b1l0.out_t.3.f.3.bias\n",
      "#params: 9437184 base_model.recurrent_model.0.layers.b1l0.mha.q_pre_f.0.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b1l0.mha.q_pre_f.1.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b1l0.mha.q_pre_f.1.bias\n",
      "#params: 18 base_model.recurrent_model.0.layers.b1l0.mha.q_sa.f.0.weight\n",
      "#params: 1 base_model.recurrent_model.0.layers.b1l0.mha.q_sa.f.0.bias\n",
      "#params: 18874368 base_model.recurrent_model.0.layers.b1l0.mha.k_pre_f.0.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b1l0.mha.k_pre_f.1.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b1l0.mha.k_pre_f.1.bias\n",
      "#params: 18 base_model.recurrent_model.0.layers.b1l0.mha.k_sa.f.0.weight\n",
      "#params: 1 base_model.recurrent_model.0.layers.b1l0.mha.k_sa.f.0.bias\n",
      "#params: 18874368 base_model.recurrent_model.0.layers.b1l0.mha.v_pre_f.0.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b1l0.mha.v_pre_f.1.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b1l0.mha.v_pre_f.1.bias\n",
      "#params: 18 base_model.recurrent_model.0.layers.b1l0.mha.v_sa.f.0.weight\n",
      "#params: 1 base_model.recurrent_model.0.layers.b1l0.mha.v_sa.f.0.bias\n",
      "#params: 1048576 base_model.recurrent_model.0.layers.b1l0.mha.v_post_f.0.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b1l0.mha.v_post_f.1.weight\n",
      "#params: 1024 base_model.recurrent_model.0.layers.b1l0.mha.v_post_f.1.bias\n",
      "#params: 4194304 base_model.recurrent_model.1.unroll.unrolling.weight_ih_l0\n",
      "#params: 4194304 base_model.recurrent_model.1.unroll.unrolling.weight_hh_l0\n",
      "#params: 4096 base_model.recurrent_model.1.unroll.unrolling.bias_ih_l0\n",
      "#params: 4096 base_model.recurrent_model.1.unroll.unrolling.bias_hh_l0\n",
      "#params: 1048576 base_model.recurrent_model.1.unroll.classifier.0.weight\n",
      "#params: 1024 base_model.recurrent_model.1.unroll.classifier.0.bias\n",
      "#params: 4194304 base_model.recurrent_model.1.unroll.fuse_ev.0.weight\n",
      "#params: 2048 base_model.recurrent_model.1.unroll.fuse_ev.0.bias\n",
      "#params: 1048576 base_model.recurrent_model.1.encoder_n.2.weight\n",
      "#params: 1024 base_model.recurrent_model.1.encoder_n.2.bias\n",
      "#params: 1048576 base_model.recurrent_model.1.encoder_v.2.weight\n",
      "#params: 1024 base_model.recurrent_model.1.encoder_v.2.bias\n",
      "#params: 1024 base_model.recurrent_model.1.c_a.0.weight\n",
      "#params: 1024 base_model.recurrent_model.1.c_a.0.bias\n",
      "#params: 1932288 base_model.recurrent_model.1.c_a.2.weight\n",
      "#params: 1887 base_model.recurrent_model.1.c_a.2.bias\n",
      "#params: 1024 base_model.recurrent_model.1.c_n.0.weight\n",
      "#params: 1024 base_model.recurrent_model.1.c_n.0.bias\n",
      "#params: 102400 base_model.recurrent_model.1.c_n.2.weight\n",
      "#params: 100 base_model.recurrent_model.1.c_n.2.bias\n",
      "#params: 1024 base_model.recurrent_model.1.c_v.0.weight\n",
      "#params: 1024 base_model.recurrent_model.1.c_v.0.bias\n",
      "#params: 51200 base_model.recurrent_model.1.c_v.2.weight\n",
      "#params: 50 base_model.recurrent_model.1.c_v.2.bias\n",
      "#params: 188700 base_model.recurrent_model.1.c_a2n.0.weight\n",
      "#params: 100 base_model.recurrent_model.1.c_a2n.0.bias\n",
      "#params: 94350 base_model.recurrent_model.1.c_a2v.0.weight\n",
      "#params: 50 base_model.recurrent_model.1.c_a2v.0.bias\n",
      "Total number of learnable parameters: 281419440\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility.\n",
    "# Set up initial random states.\n",
    "make_reproducible(random_seed=0)\n",
    "\n",
    "# Load config.\n",
    "args = parser.parse_args([])\n",
    "args.base_model = \"HORST\"\n",
    "print(args)\n",
    "\n",
    "if args.dataset_name == 'holoassist':\n",
    "    num_classes = 1887 # actions\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VideoModel(\n",
    "    num_classes=num_classes, \n",
    "    num_segments=args.num_segments, \n",
    "    base_model=args.base_model,\n",
    "    fusion_mode=args.fusion_mode,\n",
    "    dropout=args.dropout,\n",
    "    verbose=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/artemmerinov/data/backbones/FAttentionRNN-anticipation_0.25_6_8_rgb_mt5r_best.pth.tar\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/artemmerinov/data/backbones/FAttentionRNN-anticipation_0.25_6_8_rgb_mt5r_best.pth.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoModel(\n",
       "  (base_model): AttentionRNN(\n",
       "    (backbone_model): SwinBackbone(\n",
       "      (model): SwinTransformer(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (layers): Sequential(\n",
       "          (0): SwinTransformerStage(\n",
       "            (downsample): Identity()\n",
       "            (blocks): Sequential(\n",
       "              (0): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): Identity()\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): Identity()\n",
       "              )\n",
       "              (1): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.004)\n",
       "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.004)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerStage(\n",
       "            (downsample): PatchMerging(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "            )\n",
       "            (blocks): Sequential(\n",
       "              (0): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.009)\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.009)\n",
       "              )\n",
       "              (1): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.013)\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.013)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerStage(\n",
       "            (downsample): PatchMerging(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            )\n",
       "            (blocks): Sequential(\n",
       "              (0): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.017)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.017)\n",
       "              )\n",
       "              (1): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.022)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.022)\n",
       "              )\n",
       "              (2): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.026)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.026)\n",
       "              )\n",
       "              (3): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.030)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.030)\n",
       "              )\n",
       "              (4): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.035)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.035)\n",
       "              )\n",
       "              (5): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.039)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.039)\n",
       "              )\n",
       "              (6): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.043)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.043)\n",
       "              )\n",
       "              (7): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.048)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.048)\n",
       "              )\n",
       "              (8): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.052)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.052)\n",
       "              )\n",
       "              (9): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.057)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.057)\n",
       "              )\n",
       "              (10): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.061)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.061)\n",
       "              )\n",
       "              (11): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.065)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.065)\n",
       "              )\n",
       "              (12): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.070)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.070)\n",
       "              )\n",
       "              (13): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.074)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.074)\n",
       "              )\n",
       "              (14): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.078)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.078)\n",
       "              )\n",
       "              (15): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.083)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.083)\n",
       "              )\n",
       "              (16): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.087)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.087)\n",
       "              )\n",
       "              (17): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.091)\n",
       "                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.091)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerStage(\n",
       "            (downsample): PatchMerging(\n",
       "              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            )\n",
       "            (blocks): Sequential(\n",
       "              (0): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.096)\n",
       "                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.096)\n",
       "              )\n",
       "              (1): SwinTransformerBlock(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttention(\n",
       "                  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                )\n",
       "                (drop_path1): DropPath(drop_prob=0.100)\n",
       "                (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (drop1): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): Identity()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  (drop2): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path2): DropPath(drop_prob=0.100)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (head): ClassifierHead(\n",
       "          (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (fc): Linear(in_features=1024, out_features=21841, bias=True)\n",
       "          (flatten): Identity()\n",
       "        )\n",
       "        (avgpool): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (recurrent_model): Sequential(\n",
       "      (0): ConvLSTM(\n",
       "        (layers): ModuleDict(\n",
       "          (b0l0): AttentionRNNCell(\n",
       "            (frame_dropout): Dropout(p=0, inplace=False)\n",
       "            (encoder): Sequential(\n",
       "              (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(1, 1024, eps=1e-05, affine=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (out_t): Sequential(\n",
       "              (0): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(1, 1024, eps=1e-05, affine=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "              (3): ShortcutSqueezeAndExcitation(\n",
       "                (f): Sequential(\n",
       "                  (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (2): ReLU()\n",
       "                  (3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (4): Sigmoid()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (mha): STAtt(\n",
       "              (q_pre_f): Sequential(\n",
       "                (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(1, 1024, eps=1e-05, affine=True)\n",
       "              )\n",
       "              (q_sa): SpatialAttention(\n",
       "                (f): Sequential(\n",
       "                  (0): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): Sigmoid()\n",
       "                )\n",
       "              )\n",
       "              (k_pre_f): Sequential(\n",
       "                (0): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(1, 1024, eps=1e-05, affine=True)\n",
       "              )\n",
       "              (k_sa): SpatialAttention(\n",
       "                (f): Sequential(\n",
       "                  (0): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): Sigmoid()\n",
       "                )\n",
       "              )\n",
       "              (v_pre_f): Sequential(\n",
       "                (0): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(1, 1024, eps=1e-05, affine=True)\n",
       "              )\n",
       "              (v_sa): SpatialAttention(\n",
       "                (f): Sequential(\n",
       "                  (0): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): Sigmoid()\n",
       "                )\n",
       "              )\n",
       "              (v_post_f): Sequential(\n",
       "                (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): GroupNorm(1, 1024, eps=1e-05, affine=True)\n",
       "                (2): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (b1l0): AttentionRNNCell(\n",
       "            (frame_dropout): Dropout(p=0, inplace=False)\n",
       "            (encoder): Sequential(\n",
       "              (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(1, 1024, eps=1e-05, affine=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (out_t): Sequential(\n",
       "              (0): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(1, 1024, eps=1e-05, affine=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "              (3): ShortcutSqueezeAndExcitation(\n",
       "                (f): Sequential(\n",
       "                  (0): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (2): ReLU()\n",
       "                  (3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (4): Sigmoid()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (mha): STAtt(\n",
       "              (q_pre_f): Sequential(\n",
       "                (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(1, 1024, eps=1e-05, affine=True)\n",
       "              )\n",
       "              (q_sa): SpatialAttention(\n",
       "                (f): Sequential(\n",
       "                  (0): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): Sigmoid()\n",
       "                )\n",
       "              )\n",
       "              (k_pre_f): Sequential(\n",
       "                (0): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(1, 1024, eps=1e-05, affine=True)\n",
       "              )\n",
       "              (k_sa): SpatialAttention(\n",
       "                (f): Sequential(\n",
       "                  (0): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): Sigmoid()\n",
       "                )\n",
       "              )\n",
       "              (v_pre_f): Sequential(\n",
       "                (0): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (1): GroupNorm(1, 1024, eps=1e-05, affine=True)\n",
       "              )\n",
       "              (v_sa): SpatialAttention(\n",
       "                (f): Sequential(\n",
       "                  (0): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  (1): Sigmoid()\n",
       "                )\n",
       "              )\n",
       "              (v_post_f): Sequential(\n",
       "                (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): GroupNorm(1, 1024, eps=1e-05, affine=True)\n",
       "                (2): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Classifier(\n",
       "        (unroll): ActionHead(\n",
       "          (avgpool): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          )\n",
       "          (unrolling): LSTM(1024, 1024, batch_first=True)\n",
       "          (classifier): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (fuse_ev): Sequential(\n",
       "            (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder_n): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_v): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Flatten(start_dim=1, end_dim=-1)\n",
       "          (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (c_a): Sequential(\n",
       "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=1887, bias=True)\n",
       "        )\n",
       "        (c_n): Sequential(\n",
       "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=100, bias=True)\n",
       "        )\n",
       "        (c_v): Sequential(\n",
       "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=50, bias=True)\n",
       "        )\n",
       "        (c_a2n): Sequential(\n",
       "          (0): Linear(in_features=1887, out_features=100, bias=True)\n",
       "        )\n",
       "        (c_a2v): Sequential(\n",
       "          (0): Linear(in_features=1887, out_features=50, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 videos in the list There are 13 videos as video files There are 0 videos that present in the list but are missing as videos.\n",
      "Number of clips: 135 for mode train\n",
      "va_dataset\n",
      "There are 213 videos in the list There are 13 videos as video files There are 206 videos that present in the list but are missing as videos.\n",
      "Number of clips: 135 for mode validation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "crop_size = model.crop_size\n",
    "scale_size = model.scale_size\n",
    "input_mean = model.input_mean\n",
    "input_std = model.input_std\n",
    "div = model.div\n",
    "learnable_named_parameters = model.learnable_named_parameters\n",
    "\n",
    "# Parallel!\n",
    "# model = torch.nn.DataParallel(model)\n",
    "# model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "#  ========================= TRAIN DATA =========================\n",
    "# \n",
    "\n",
    "print(\"tr_dataset\", flush=True)\n",
    "\n",
    "tr_clip_path_to_video_arr, tr_clip_start_arr, tr_clip_end_arr, tr_clip_action_id_arr, _ = prepare_clips_data(\n",
    "    raw_annotation_file=args.raw_annotation_file,\n",
    "    holoassist_dir=args.holoassist_dir,\n",
    "    split_dir=args.split_dir,\n",
    "    fine_grained_actions_map_file=args.fine_grained_actions_map_file,\n",
    "    mode=\"train\",\n",
    ")\n",
    "tr_transform = Compose([\n",
    "    GroupMultiScaleCrop(input_size=crop_size, scales=[1, .875]),\n",
    "    Stack(),\n",
    "    ToTorchFormatTensor(div=div),\n",
    "    GroupNormalize(mean=input_mean, std=input_std),\n",
    "])\n",
    "\n",
    "tr_dataset = VideoDataset(\n",
    "    clip_path_to_video_arr=tr_clip_path_to_video_arr,\n",
    "    clip_start_arr=tr_clip_start_arr,\n",
    "    clip_end_arr=tr_clip_end_arr,\n",
    "    clip_label_arr=tr_clip_action_id_arr,\n",
    "    num_segments=args.num_segments,\n",
    "    transform=tr_transform,\n",
    "    mode=\"train\"\n",
    ")\n",
    "tr_dataloader = DataLoader(\n",
    "    dataset=tr_dataset, \n",
    "    batch_size=args.batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers, \n",
    "    drop_last=True,\n",
    "    pin_memory=False,\n",
    "    prefetch_factor=args.prefetch_factor,\n",
    "    # pin_memory=True,\n",
    ")\n",
    "\n",
    "#  ========================= VALIDATION DATA =========================\n",
    "\n",
    "print(\"va_dataset\", flush=True)\n",
    "\n",
    "va_clip_path_to_video_arr, va_clip_start_arr, va_clip_end_arr, va_clip_action_id_arr, _ = prepare_clips_data(\n",
    "    raw_annotation_file=args.raw_annotation_file,\n",
    "    holoassist_dir=args.holoassist_dir, \n",
    "    split_dir=args.split_dir,\n",
    "    fine_grained_actions_map_file=args.fine_grained_actions_map_file,\n",
    "    mode=\"validation\",\n",
    ")\n",
    "va_transform = Compose([\n",
    "    GroupMultiScaleCrop(input_size=crop_size, scales=[1, .875]),\n",
    "    Stack(),\n",
    "    ToTorchFormatTensor(div=div),\n",
    "    GroupNormalize(mean=input_mean, std=input_std),\n",
    "])\n",
    "va_dataset = VideoDataset(\n",
    "    clip_path_to_video_arr=va_clip_path_to_video_arr,\n",
    "    clip_start_arr=va_clip_start_arr,\n",
    "    clip_end_arr=va_clip_end_arr,\n",
    "    clip_label_arr=va_clip_action_id_arr,\n",
    "    num_segments=args.num_segments,\n",
    "    transform=va_transform,\n",
    "    mode=\"validation\"\n",
    ")\n",
    "va_dataloader = DataLoader(\n",
    "    dataset=va_dataset, \n",
    "    batch_size=args.batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers, \n",
    "    drop_last=False, \n",
    "    pin_memory=False,\n",
    "    prefetch_factor=args.prefetch_factor,\n",
    "    # pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    params=model.parameters(),\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum,\n",
    "    weight_decay=args.weight_decay\n",
    ")\n",
    "lr_scheduler = CosineAnnealingLR(\n",
    "    optimizer=optimizer, \n",
    "    T_max=args.num_epochs, \n",
    "    eta_min=1e-7, \n",
    "    last_epoch=-1\n",
    ")\n",
    "\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "\n",
    "        # Load checkpoint file that contains all the states\n",
    "        print(f\"=> Loading checkpoint {args.resume}\")\n",
    "        checkpoint = torch.load(f=args.resume)\n",
    "\n",
    "        # Load state from checkpoint\n",
    "        model.load_state_dict(state_dict=checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(state_dict=checkpoint['optimizer_state_dict'])\n",
    "        lr_scheduler.load_state_dict(state_dict=checkpoint['lr_scheduler_state_dict'])\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n",
    "    else:\n",
    "        raise ValueError(f\"=> No checkpoint found at {args.resume}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making reproducible on seed 0\n",
      "\n",
      "Epoch 0 LR=0.0100000 time=2024-05-02 22:11:58\n",
      "\n",
      "TRAIN\n",
      "tr_batch_id=0000/0008 tr_batch_loss=7.715 tr_batch_acc@1=0.000 tr_batch_acc@5=0.000 | tr_epoch_loss=7.715 tr_epoch_acc@1=0.000 tr_epoch_acc@5=0.000\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# ==================== Main train-validation loop =================================\n",
    "\n",
    "for epoch in range(args.start_epoch, args.num_epochs):\n",
    "\n",
    "    # Reproducibility.\n",
    "    # Set up random seed to current epoch\n",
    "    # This is important to preserve reproducibility \n",
    "    # in case when we load model checkpoint.\n",
    "    make_reproducible(random_seed=epoch)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}\", \n",
    "            f\"LR={optimizer.param_groups[0]['lr']:.7f}\",\n",
    "            f\"time={datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "            flush=True)\n",
    "\n",
    "    # TRAIN\n",
    "    # =====\n",
    "    print(f\"\\nTRAIN\")\n",
    "\n",
    "    tr_epoch_loss = AverageMeter()\n",
    "    tr_epoch_acc1 = AverageMeter()\n",
    "    tr_epoch_acc5 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    for tr_batch_id, tr_batch in enumerate(tr_dataloader):\n",
    "        \n",
    "        tr_x = tr_batch[0].to(device) # video batch with image sequences [n, t_c, h, w]\n",
    "        tr_y = tr_batch[1].to(device) # video batch labels [n]\n",
    "\n",
    "        # Make predictions for train batch\n",
    "        tr_preds = model(tr_x)\n",
    "        tr_loss = criterion(tr_preds, tr_y)\n",
    "        tr_acc1, tr_acc5  = calc_accuracy(preds=tr_preds, labels=tr_y, topk=(1,5))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Compute gradient of the loss wrt all learnable parameters\n",
    "        tr_loss.backward()\n",
    "\n",
    "        # Clip computed gradients\n",
    "        if args.clip_gradient is not None:\n",
    "            total_norm = clip_grad_norm_(parameters=model.parameters(), max_norm=args.clip_gradient)\n",
    "            # if total_norm > args.clip_gradient:\n",
    "            #     print(f\"Clipping gradient: {total_norm} with coef {args.clip_gradient / total_norm}\")\n",
    "        \n",
    "        # Update the weights using optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Keep track of epoch metrics (for each batch)\n",
    "        tr_epoch_loss.update(value=tr_loss.detach().item(), n=tr_x.size(0))\n",
    "        tr_epoch_acc1.update(value=tr_acc1, n=tr_x.size(0))\n",
    "        tr_epoch_acc5.update(value=tr_acc5, n=tr_x.size(0))\n",
    "\n",
    "        if tr_batch_id % 20 == 0:\n",
    "\n",
    "            print(f\"tr_batch_id={tr_batch_id:04d}/{len(tr_dataloader):04d}\",\n",
    "                    f\"tr_batch_loss={tr_loss.detach().item():.3f}\",\n",
    "                    f\"tr_batch_acc@1={tr_acc1:.3f}\",\n",
    "                    f\"tr_batch_acc@5={tr_acc5:.3f}\",\n",
    "                    f\"|\",\n",
    "                    f\"tr_epoch_loss={tr_epoch_loss.avg:.3f}\",\n",
    "                    f\"tr_epoch_acc@1={tr_epoch_acc1.avg:.3f}\",\n",
    "                    f\"tr_epoch_acc@5={tr_epoch_acc5.avg:.3f}\",\n",
    "                    flush=True)\n",
    "            \n",
    "        del tr_preds, tr_loss, tr_acc1, tr_acc5, tr_batch, tr_x, tr_y\n",
    "        gc.collect()\n",
    "        # torch.cuda.empty_cache() # expensive call\n",
    "    \n",
    "    # Adjust learning rate after training epoch\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
